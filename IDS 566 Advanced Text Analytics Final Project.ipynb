{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDS 566\n",
    "### Advanced Text Analytics\n",
    "### Final Project - The 20 Newsgroup Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitted by\n",
    "* Palak Gupta - 677408458\n",
    "* Ashwyn Muthukumarswami - 675870750\n",
    "* Gughanraj Selvaraju - 662075168\n",
    "* Meenakshi Janardanam - 671917004\n",
    "* Aruna Venkatasubramaniam - 678860489"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "data_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the number of rows in train and test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314 documents\n",
      "20 categories\n",
      "7532 documents\n",
      "20 categories\n"
     ]
    }
   ],
   "source": [
    "print(\"%d documents\" % len(data_train.filenames))\n",
    "print(\"%d categories\" % len(data_train.target_names))\n",
    "\n",
    "print(\"%d documents\" % len(data_test.filenames))\n",
    "print(\"%d categories\" % len(data_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    0.053032\n",
       "15    0.052943\n",
       "8     0.052855\n",
       "9     0.052766\n",
       "11    0.052590\n",
       "13    0.052501\n",
       "7     0.052501\n",
       "14    0.052413\n",
       "5     0.052413\n",
       "12    0.052236\n",
       "2     0.052236\n",
       "3     0.052148\n",
       "6     0.051706\n",
       "1     0.051617\n",
       "4     0.051087\n",
       "17    0.049850\n",
       "16    0.048259\n",
       "0     0.042425\n",
       "18    0.041100\n",
       "19    0.033322\n",
       "Name: Names, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split a training set and a test set\n",
    "y_train, y_test = data_train.target, data_test.target\n",
    "\n",
    "## Label Distribution in Training\n",
    "ls = pd.DataFrame(y_train, columns =['Names'])\n",
    "ls['Names'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data to remove numeric and alphanumeric strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train_text = []\n",
    "test_text = []\n",
    "for text in data_train.data:\n",
    "    train_text.append(re.sub(r'[^a-zA-Z\\s]+','',text))\n",
    "for text1 in data_test.data:\n",
    "    test_text.append(re.sub(r'[^a-zA-Z\\s]+','',text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"data\"] = train_text\n",
    "data_test[\"data\"] = test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming the words to get the root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split(' ')])\n",
    "\n",
    "def lemma_words(text):\n",
    "    lemmer=WordNetLemmatizer()\n",
    "    return ' '.join([lemmer.lemmatize(word) for word in text.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_text = []\n",
    "lemmed_text = []\n",
    "for text in data_train.data:\n",
    "    stemmed_text.append(stem_words(text))\n",
    "    lemmed_text.append(lemma_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"stemmed_data\"] = stemmed_text\n",
    "data_train[\"lemmed_data\"] = lemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"> We analysed the pre defined structure of the input data from scikit learn library and added the stemmed text as the part of the pre defined structure. This can be ran only once and reused in the multiple stages in the model building. We arrived at this conclusion after experimenting the stemming module to be part of the model building pipeline which increased the model training time exponentially </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_text = []\n",
    "lemmed_text = []\n",
    "for text in data_test.data:\n",
    "    stemmed_text.append(stem_words(text))\n",
    "    lemmed_text.append(lemma_words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test[\"stemmed_data\"] = stemmed_text\n",
    "data_test[\"lemmed_data\"] = lemmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 110877)\n",
      "(7532, 110877)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = count_vect = CountVectorizer(stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(data_train.stemmed_data)\n",
    "X_test_counts = count_vect.transform(data_test.stemmed_data)\n",
    "print(X_train_counts.shape)\n",
    "print(X_test_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 65928)\n",
      "(11314, 65928)\n"
     ]
    }
   ],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(sublinear_tf=True, max_df = 1,stop_words='english')\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(data_train.stemmed_data)\n",
    "X_test_tfidf = vectorizer_tfidf.transform(data_train.stemmed_data)\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65928"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Getting the total feature vextor length i.e. vocabulary size\n",
    "feature_names_tfidf = vectorizer_tfidf.get_feature_names()\n",
    "len(feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 - Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes with CountVectorizer and unstemmed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB = MultinomialNB().fit(X_train_counts, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - No Tuning - No Stemming\n",
      "Training Accuracy: 0.9695068057274173\n",
      "Training Accuracy: 0.8108072225172597\n"
     ]
    }
   ],
   "source": [
    "NB_predicted_train = NB.predict(X_train_counts)\n",
    "NB_predicted = NB.predict(X_test_counts)\n",
    "\n",
    "print(\"Naive Bayes - No Tuning - No Stemming\")\n",
    "print(\"Training Accuracy:\", np.mean(NB_predicted_train == y_train))\n",
    "print(\"Training Accuracy:\", np.mean(NB_predicted == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes using pipeling with stemmed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), ('mnb', MultinomialNB(fit_prior=False))])\n",
    "\n",
    "text_mnb_stemmed = text_mnb_stemmed.fit(data_train.data, y_train)\n",
    "\n",
    "predicted_mnb_stemmed = text_mnb_stemmed.predict(data_test.data)\n",
    "predicted_mnb_stemmed_train = text_mnb_stemmed.predict(data_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes -No Tuning - Stemming\n",
      "Training Accuracy: 0.9528018384302633\n",
      "Test Accuracy: 0.8190387679235263\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes -No Tuning - Stemming\")\n",
    "print(\"Training Accuracy:\", np.mean(predicted_mnb_stemmed_train == y_train))\n",
    "print(\"Test Accuracy:\",np.mean(predicted_mnb_stemmed == y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the model with stemming and tfidf performes better. For all the models going forwards therefore we used stemmed data and tfidf vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tunning - Naive Bayes - using the GridSearch without Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-1, 1e-2, 1e-3,1e-4), }\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(data_train.data, y_train)\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(data_train.data, data_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'> Pipeline and grid search cross validation is performed and paramters(n_jobs = -1) enables us to utilize multiple cores in the system</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_\n",
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_gs_clf = gs_clf.predict(data_test.data)\n",
    "predicted_gs_clf_train = gs_clf.predict(data_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes -With Tuning - Without Stemming\n",
      "Training Accuracy: 0.8509028146574615\n",
      "Test Accuracy: 0.9993812975075128\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes -With Tuning - Without Stemming\")\n",
    "print(\"Training Accuracy:\",np.mean(predicted_gs_clf == y_test))\n",
    "print(\"Test Accuracy:\",np.mean(predicted_gs_clf_train == y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tunning - Naive Bayes - using the GridSearch with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-1, 1e-2, 1e-3,1e-4), }\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(data_train.stemmed_data, y_train)\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(data_train.stemmed_data, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Finding the best score and best parameters\n",
    "print(gs_clf.best_score_)\n",
    "print(gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Prodiction on test and train data\n",
    "predicted_gs_clf_train = gs_clf.predict(data_train.stemmed_data)\n",
    "predicted_gs_clf = gs_clf.predict(data_test.stemmed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes -With Tuning - Stemming\n",
      "Training Accuracy: 0.9992045253668022\n",
      "Test Accuracy: 0.8315188528943176\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes -With Tuning - Stemming\")\n",
    "print(\"Training Accuracy:\",np.mean(predicted_gs_clf_train == y_train))\n",
    "print(\"Test Accuracy:\",np.mean(predicted_gs_clf == y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - 2 Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM model with without stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(data_train.stemmed_data, data_train.target)\n",
    "### Predition on test and train data\n",
    "predicted_svm_train = text_clf_svm.predict(data_train.stemmed_data)\n",
    "predicted_svm = text_clf_svm.predict(data_test.stemmed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine -No Tuning - No Stemming\n",
      "Training Accuracy: 0.9613752872547286\n",
      "Test Accuracy: 0.8158523632501328\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Support Vector Machine -No Tuning - No Stemming\")\n",
    "print(\"Training Accuracy:\",np.mean(predicted_svm_train == data_train.target))\n",
    "print(\"Test Accuracy:\", np.mean(predicted_svm == data_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning Using SVM - Without Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(data_train.data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf-svm__alpha': 0.0001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf-svm__alpha': (1e-1, 1e-2, 1e-3,1e-4 ), }\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(data_train.data, data_train.target)\n",
    "\n",
    "### Finding the best score and best parameters\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predition on test and train data\n",
    "glf_predicted_svm_train = gs_clf_svm.predict(data_train.data)\n",
    "glf_predicted_svm = gs_clf_svm.predict(data_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine -With Tuning - No Stemming\n",
      "Training Accuracy: 0.9972600318189854\n",
      "Test Accuracy: 0.8555496548061604\n"
     ]
    }
   ],
   "source": [
    "print(\"Support Vector Machine -With Tuning - No Stemming\")\n",
    "print(\"Training Accuracy:\",np.mean(glf_predicted_svm_train == data_train.target))\n",
    "print(\"Test Accuracy:\",np.mean(glf_predicted_svm == data_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning Using SVM - Using Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, n_iter=5, random_state=42))])\n",
    "\n",
    "text_clf_svm = text_clf_svm.fit(data_train.stemmed_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'clf-svm__alpha': 0.0001, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters_svm = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf-svm__alpha': (1e-1, 1e-2, 1e-3,1e-4 ), }\n",
    "\n",
    "gs_clf_svm = GridSearchCV(text_clf_svm, parameters_svm, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(data_train.stemmed_data, data_train.target)\n",
    "\n",
    "### Finding the best score and best parameters\n",
    "gs_clf_svm.best_score_\n",
    "gs_clf_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction on train and test data\n",
    "glf_predicted_svm_train = gs_clf_svm.predict(data_train.stemmed_data)\n",
    "glf_predicted_svm = gs_clf_svm.predict(data_test.stemmed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine -With Tuning - with Stemming\n",
      "Training Accuracy: 0.9925755700901538\n",
      "Training Accuracy: 0.8166489644184811\n"
     ]
    }
   ],
   "source": [
    "print(\"Support Vector Machine -With Tuning - with Stemming\")\n",
    "print(\"Training Accuracy:\",np.mean(glf_predicted_svm_train == data_train.target))\n",
    "print(\"Training Accuracy:\",np.mean(glf_predicted_svm == data_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3  - Logistic Regression with Regularization - Ridge and Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning without Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words='english',preprocessor=None)\n",
    "vectorizer_tfidf = TfidfTransformer()\n",
    "\n",
    "model = Pipeline([(\"CVectorizer\",count_vect),(\"TF_IDF\",vectorizer_tfidf),(\"SGDC\",SGDClassifier(loss='log',max_iter=1000,tol=1e-3))])\n",
    "parameters = {\n",
    "    'CVectorizer__max_df':(0.1,),\n",
    "    'CVectorizer__ngram_range': [(1, 1), (1, 2)],\n",
    "    'SGDC__penalty':('l1','l2'),\n",
    "    'SGDC__alpha':(0.1,0.01,0.0001,1e-5,1e-10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('CVectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       " ..._state=None, shuffle=True, tol=0.001,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'CVectorizer__max_df': (0.1,), 'CVectorizer__ngram_range': [(1, 1), (1, 2)], 'SGDC__penalty': ('l1', 'l2'), 'SGDC__alpha': (0.1, 0.01, 0.0001, 1e-05, 1e-10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSCV = GridSearchCV(model, parameters,n_jobs=-1)\n",
    "GSCV.fit(data_train.data,data_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CVectorizer__max_df': 0.1,\n",
       " 'CVectorizer__ngram_range': (1, 2),\n",
       " 'SGDC__alpha': 1e-05,\n",
       " 'SGDC__penalty': 'l2'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSCV.best_score_\n",
    "GSCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = GSCV.predict(data_train.data)\n",
    "preds = GSCV.predict(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9994696835778681\n",
      "0.8566117896972916\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(preds_train == data_train.target))\n",
    "print(np.mean(preds == data_test.target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning with Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palak\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('CVectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       " ..._state=None, shuffle=True, tol=0.001,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'CVectorizer__max_df': (0.1,), 'CVectorizer__ngram_range': [(1, 1), (1, 2)], 'SGDC__penalty': ('l1', 'l2'), 'SGDC__alpha': (0.1, 0.01, 0.0001, 1e-05, 1e-10)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSCV = GridSearchCV(model, parameters,n_jobs=-1)\n",
    "GSCV.fit(data_train.stemmed_data ,data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CVectorizer__max_df': 0.1,\n",
       " 'CVectorizer__ngram_range': (1, 2),\n",
       " 'SGDC__alpha': 1e-05,\n",
       " 'SGDC__penalty': 'l2'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSCV.best_score_\n",
    "GSCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = GSCV.predict(data_train.stemmed_data)\n",
    "preds = GSCV.predict(data_test.stemmed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9993812975075128\n",
      "0.8516994158258099\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(preds_train == data_train.target))\n",
    "print(np.mean(preds == data_test.target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
